# -*- coding: utf-8 -*-
"""Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rSkRTekmFmvofG4gJMhTIUcYPBsmib8E
"""

# Importing the neccesarries libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error
import os

# Importing the data from local
bnkA = pd.read_csv('bank-additional-full.csv', delimiter = ';')

bnkA.head(10)

# Getting the dimensions of the dataset
bnkA.shape

# Getting the information
bnkA.info()

# Replacing unknown values or the place holders with the NULL
bnkA['pdays'] = bnkA['pdays'].replace(999,np.nan)
bnkA = bnkA.replace('unknown',np.nan)

# getting the count of null values
(bnkA.isnull().sum()/len(bnkA))

# Dropping columns with 80% of the records with null values
bnkA = bnkA.dropna(axis = 1, thresh = len(bnkA)*0.80)

bnkA.head(10)

#dropping null in all columns
bnkA= bnkA.dropna()

# getting the count of null values after removing the null values
(bnkA.isnull().sum()/len(bnkA))*100

# Replacing the special characters
bnkA['job'] = bnkA['job'].replace('.',' ')

bnkA.describe()

#cartegorical columns
catVar = bnkA.columns[bnkA.dtypes == 'object'].tolist()
#numerical columns
numVar = bnkA.columns[bnkA.dtypes != 'object'].tolist()
catVar.remove('y')

# Plotting the distribution of the numerical columns
fig, ax = plt.subplots(3,3, figsize=(10,10))
index = 0
for i in range(3):
  for j in range(3):
    ax[i,j].hist(bnkA[numVar[index]], edgecolor='black') # plotting the histogram
    ax[i,j].set_title("Distribution of "+str(numVar[index])) # setting the titles
    ax[i,j].set_xlabel('Bins')
    index = index + 1
plt.tight_layout()
plt.show()

# Getting the count w.r.t job discription
bnkA.groupby('job')['age'].count()

# Plotting the distribution og categorical columns
fig, ax = plt.subplots(3,3, figsize=(10,10))
index = 0
for i in range(3):
  for j in range(3):
    t = bnkA.groupby(catVar[index], as_index=False)['age'].count() # Getting the counts
    ax[i,j].bar(t[catVar[index]],t['age']) #plotting the bar plots
    ax[i,j].set_title("Distribution of "+str(catVar[index])) # settingg the title
    # ax[i,j].set_xlabel('Bins')
    index = index + 1
plt.tight_layout()
plt.show()

# Label Encoding - for categorical variables
enc = LabelEncoder()

# creating a copy
bnkCheck = bnkA.copy()

bnkCheck['y'] = enc.fit_transform(bnkCheck['y'])

# Using encoder to convert categorical columns to binary columns
oneEnc = pd.get_dummies(bnkCheck[['poutcome','marital','education']])
dfEnc = pd.concat([bnkCheck,oneEnc],axis=1)
dfEnc.drop(['poutcome','marital','education','job'],axis=1,inplace=True)

catVar2 = dfEnc.columns[dfEnc.dtypes == 'object'].tolist()

for i in catVar2:
  dfEnc[i] = enc.fit_transform(dfEnc[i])

dfEnc.drop(['day_of_week','month','duration'],axis=1,inplace=True)

dfEnc.head(10)

# Correlation heatmap
fig, hm = plt.subplots(figsize=(10,10))
sns.heatmap(bnkCheck.corr(),annot=True,cmap='Reds')

# Getting a correlation matrix and getting all the columns which have 100% correlated with other
corMat = bnkCheck.corr().abs() # getting the correlation matrix
upper = corMat.where(np.triu(np.ones(corMat.shape), k=1).astype(np.bool)) # making the lower traingle null
dropCol = [i for i in upper.columns if any(upper[i] >= 0.9)] # getting one of the columns to drop if they are same

dropCol

dropCol = dropCol + ['duration']

bnkA = bnkA.drop(dropCol, axis = 1)

dfEnc

"""Building Models"""

# Importing the neccesary libraries
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score,precision_score, recall_score
from sklearn.metrics import confusion_matrix

X = dfEnc.drop(['y'],axis=1)
y = dfEnc['y'].values.reshape(-1, 1)

xTrain,xTest,yTrain,yTest = train_test_split(X,y,test_size=0.25,random_state=32)

"""Decision Tree"""

# Importing decision tree modules
from sklearn.model_selection import GridSearchCV

dTr = DecisionTreeClassifier(min_samples_leaf=2,max_leaf_nodes=8,max_depth=3)

dTr.fit(xTrain,yTrain)

yPred1 = dTr.predict(xTest)

#Getting Accuracy, Precision, Recall
acc1 = accuracy_score(yTest,yPred1)
pre1 = precision_score(yTest,yPred1)
rec1 = recall_score(yTest,yPred1)
print(f"ACCURACY for Decision Trees:{acc1:.3f}")
print(f"PRECISION :{pre1:.3f} and RECALL : {rec1:.3f}")

# Root Mean Squared Error (RMSE)
rmse1 = np.sqrt(mean_squared_error(yTest,yPred1))
print(f"RMSE for Decision Trees model: {rmse1:.3f}")

# Mean Absolute Error (MAE)
mae1 = mean_absolute_error(yTest,yPred1)
print(f"MAE for Decision Trees model: {mae1:.3f}")

# Getting the confusion matrix
confMat1 = confusion_matrix(yTest,yPred1)
confMat1

confMat1od = confMat1.ravel()

confMat1od_str = [f'{count:.0f}' for count in confMat1od]

confMat_dec = np.array(confMat1od_str).reshape((2, 2))

xLbls = ['True Negative', 'False Positive']
yLbls = ['True Positive', 'False Negative']

sns.heatmap(confMat1, annot=confMat_dec, fmt='', cmap='Reds', xticklabels=xLbls, yticklabels=yLbls)

"""Logistic Regression"""

# Importing Logistic Regression Modules
logReg = LogisticRegression()

logReg.fit(xTrain,yTrain)

yPred2 = logReg.predict(xTest)

#Getting Accuracy, Precision, Recall
acc2 = accuracy_score(yTest,yPred2)
pre2 = precision_score(yTest,yPred2)
rec2 = recall_score(yTest,yPred2)
print(f"ACCURACY for Logistic Regression:{acc2:.3f}")
print(f"PRECISION :{pre2:.3f} and RECALL : {rec2:.3f}")

# Root Mean Squared Error (RMSE)
rmse2 = np.sqrt(mean_squared_error(yTest,yPred2))
print(f"RMSE for Logistic Regression model: {rmse2:.3f}")

# Mean Absolute Error (MAE)
mae2 = mean_absolute_error(yTest,yPred2)
print(f"MAE for Logistic Regression model: {mae2:.3f}")

# Getting the confusion matrix
confMat2 = confusion_matrix(yTest,yPred2)
confMat2

confMat2od = confMat2.ravel()

confMat2od_str = [f'{count:.0f}' for count in confMat2od]

confMat_dec = np.array(confMat2od_str).reshape((2, 2))

xLbls = ['True Negative', 'False Positive']
yLbls = ['True Positive', 'False Negative']

sns.heatmap(confMat1, annot=confMat_dec, fmt='', cmap='Reds', xticklabels=xLbls, yticklabels=yLbls)

"""Support Vector Machine"""

# Importing Support Vector Machine modules
supVecMac = SVC(kernel='linear')

supVecMac.fit(xTrain,yTrain)

yPred3 = supVecMac.predict(xTest)

#Getting Accuracy, Precision, Recall
acc3 = accuracy_score(yTest,yPred3)
pre3 = precision_score(yTest,yPred3)
rec3 = recall_score(yTest,yPred3)
print(f"ACCURACY for Support Vector Machine:{acc3:.3f}")
print(f"Precision :{pre3:.3f} and Recall : {rec3:.3f}")

# Root Mean Squared Error (RMSE)
rmse3 = np.sqrt(mean_squared_error(yTest,yPred3))
print(f"RMSE for Logistic Regression model: {rmse3:.3f}")

# Mean Absolute Error (MAE)
mae3 = mean_absolute_error(yTest,yPred3)
print(f"MAE for Logistic Regression model: {mae3:.3f}")

# Getting the confusion matrix
confMat3 = confusion_matrix(yTest,yPred3)
confMat3

confMat3od = confMat3.ravel()

confMat3od_str = [f'{count:.0f}' for count in confMat3od]

confMat_dec = np.array(confMat3od_str).reshape((2, 2))

xLbls = ['True Negative', 'False Positive']
yLbls = ['True Positive', 'False Negative']

sns.heatmap(confMat3, annot=confMat_dec, fmt='', cmap='Reds', xticklabels=xLbls, yticklabels=yLbls)

models = [dTr, logReg, supVecMac]

yPred = [yPred1,yPred2, yPred3]
model= ['Decision Tree Classifier','Logistic Regression','SVM']

plt.figure(figsize=(10,6))
for i in range(0,3):
  yPred2 = yPred[i]
  model2 = model[i]
  fpr,tpr,threshold = metrics.roc_curve(yTest,yPred2)
  auc = metrics.auc(fpr,tpr)
  print(fpr,tpr)
  plt.plot(fpr, tpr, label=f"{model2}:(AUC = {auc:.3f})")
plt.legend()

Accuracy = [acc1,acc2,acc3]
Precision = [pre1,pre2,pre3]
recall = [rec1,rec2,rec3]

model= ['Decision Tree Classifier','Logistic Regression','SVM']

data = {'Models': model, 'Accuracy': Accuracy, 'Precision': Precision, 'Recall': recall}
df = pd.DataFrame(data)

df